{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Job Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn this on Izzy Analytics's Youtube channel, video link attached below, cheers!\n",
    "#https://www.youtube.com/watch?v=eN_3d4JrL_w&list=LL&index=4\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_url(position, location, days):\n",
    "    \"\"\"Generate url from position and location\"\"\"\n",
    "    template = 'https://www.indeed.com/jobs?q={}&l={}&fromage={}'\n",
    "    position = position.replace(' ', '+')\n",
    "    location = location.replace(' ', '+')\n",
    "    url = template.format(position, location, days)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_record(card):\n",
    "    \"\"\"Extract job data from a single record\"\"\"\n",
    "    \n",
    "    job_title = card.h2.a.get('title')\n",
    "    company = card.find('span', 'company').text.strip()\n",
    "    job_location = card.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    post_date = card.find('span', 'date').text\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    summary = card.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    job_url = 'https://www.indeed.com' + card.h2.a.get('href')\n",
    "\n",
    "    # this does not exists for all jobs, so handle the exceptions\n",
    "    salary_tag = card.find('span', 'salaryText')\n",
    "    if salary_tag:\n",
    "        salary = salary_tag.text.strip()\n",
    "    else:\n",
    "        salary = ''  \n",
    "        \n",
    "    record = (job_title, company, job_location, post_date, today, summary, salary, job_url)\n",
    "    return record\n",
    "\n",
    "\n",
    "def main(position, location, days):\n",
    "    \"\"\"Run the main program routine\"\"\"\n",
    "    records = []\n",
    "    url = get_url(position, location, days)\n",
    "    \n",
    "    # extract the job data\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.find_all('div', 'jobsearch-SerpJobCard')\n",
    "        for card in cards:\n",
    "            record = get_record(card)\n",
    "            records.append(record)\n",
    "        try:\n",
    "            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "        except AttributeError:\n",
    "            break\n",
    "        \n",
    "    # save the job data\n",
    "    with open('jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['JobTitle', 'Company', 'Location', 'PostDate', 'ExtractDate', 'Summary', 'Salary', 'JobUrl'])\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try different job titles, Locations, days posted  yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching for business intelligence analyst in MA posted with 7 days\n",
    "main('business intelligence analyst','Massachusetts','7')\n",
    "#jobs.csv file has been saved to your local folder after you run the code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geting Detailed job descriptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the job summary texts in the last file are shortened and if you wanna get the full job description here is the code to do it. However, I'm not using Indeed's API and this code might get blocked by Indeed for too many accessing times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing everything again just in case you want to make it into another seperate notebook\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...\n",
       "1    https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...\n",
       "2    https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...\n",
       "3    https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...\n",
       "4    https://www.indeed.com/rc/clk?jk=225bdb0759ee1...\n",
       "Name: JobUrl, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting just urls for scrapping\n",
    "#Using the jobs.csv your just scrapped\n",
    "df = pd.read_csv('jobs.csv')\n",
    "urldf = df['JobUrl']\n",
    "urldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count many jobs in this csv file\n",
    "len(urldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINANCE\\nANALYST, BUSINESS INTELLIGENCE\\nWestford, Massachusetts, United States of America\\n\\nSPEED & SPIRIT is what we look for in our candidates, defined by some simple values that inspire us to BE DRIVEN in our performance, BE VIBRANT in our sporting legacy, BE TOGETHER in our team spirit, and BE YOU to let our individual talent and experience shine. Applying for a job at PUMA is easy and all genders are welcome. Simply click APPLY ONLINE and follow the steps to upload your application.\\nYOUR MISSION\\nConceptualize, develop, implement, and maintain reporting solutions for monitoring and analyzing business performance for wide audiences\\nMaintain Power BI visualization suite including report access, building new reports, dashboards, and visualization solutions, and app creation while adhering to reporting standards and best practices\\nWork with end users by conducting ad hoc reporting, supporting special projects, leveraging data to identify and tell a story around the root cause of problems, and spearhead resolutions with a focus in reducing costs, increasing revenue, and improving processes\\nTranslate end-user requirements into technical specifications for development and BI teams\\nCollaborate cross-functionally to improve current operations reporting to improve efficiency and provide recommendations on new data to capture for future analysis\\nApply various statistical modeling techniques to answer business questions; validate predictions and communicate findings clearly to all levels of the organization\\nYOUR TALENT\\n2-3 years of related experience\\nBA/BS in Finance, Economics, Mathematics, Business, Analytics, or quantitative study\\nExcel skills including Power Query, Power Pivot or data modeling, and experience working with SSAS tabular cubes\\nPower BI or related experience in an enterprise setting including report building, implementation and management of row-level-security, distribution of apps, reports, and datasets, and DAX skills\\nExperience with scripting languages such as R or Python\\nAbility to work one-on-one with end users to discuss enhancements and resolve issues by providing actionable recommendations based on data\\nExperience with Microsoft Power Platform synergies (Power Automate, Power BI, Power Apps), Erwin Data Catalog, and familiarity of SCRUM framework in a business setting while managing backlog items\\nTeam player who can take initiative ability to meet deadlines, and effectively work cross-functionally.\\nPUMA supports over 14,000 employees across 120+ countries. The PUMA Group owns the brand PUMA, Cobra Golf and stichd, and is headquartered in Herzogenaurach, Germany.\\nPUMA is an Equal Employment Opportunity (EEO) employer. It is the policy of PUMA to prohibit discrimination and harassment of any type and to afford equal employment opportunities to all persons without regard to race, color, religion, sex, national origin, age, gender, physical or mental disability, veteran-status, or any other characteristic protected by applicable federal, state or local law.For additional information, please contact: us-hrrecruiter@puma.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try one random example\n",
    "response_fulljd = requests.get('https://www.indeed.com/rc/clk?jk=225bdb0759ee17be&fccid=2f6f2e23480ffded&vjs=3')\n",
    "soup_fulljd = BeautifulSoup(response_fulljd.text, 'html.parser')\n",
    "jd = soup_fulljd.find('div', 'jobsearch-jobDescriptionText')\n",
    "fjd=jd.text.strip()\n",
    "fjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the text and make into a list\n",
    "joblist= []\n",
    "for u in urldf:\n",
    "    response_fulljd = requests.get(u)\n",
    "    soup_fulljd = BeautifulSoup(response_fulljd.text, 'html.parser')\n",
    "    jd = soup_fulljd.find('div', 'jobsearch-jobDescriptionText')\n",
    "    fjd=jd.text.strip()\n",
    "    fjd_clean = fjd.replace('\\n', ' ')\n",
    "    joblist.append(fjd_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count job descriptions number\n",
    "len(joblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn list into dataframe and save into a csv file with Urls aside\n",
    "joblist_df = pd.DataFrame(joblist,columns=['Full Job Description'])\n",
    "joblist_df['JobUrl'] = urldf\n",
    "joblist_df.to_csv('Full Job Descriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Job Descriptions.csv saved to local folder\n",
    "# Use Wrap Text function in excel to read clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
